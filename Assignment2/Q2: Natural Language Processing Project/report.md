# Natural Language Processing Project Report

## Introduction

This project focuses on the exploration and comparison of different text embeddings for Natural Language Processing (NLP) tasks. Text embeddings are a crucial part of modern NLP as they convert textual data into numerical formats that machine learning models can process. The embeddings examined in this project are **Word2Vec**, **GloVe**, and **BERT**. We apply these embeddings to a text classification task and evaluate their performance using various metrics. Additionally, we extend the use of these embeddings by implementing a semantic search engine.

## Methodology

### Data Collection
The dataset used for this project is related to climate change, comprising textual descriptions and corresponding labels indicating whether the text is positive or negative towards climate action.

### Text Embeddings
1. **Word2Vec**: Word2Vec is a popular word embedding technique that uses a neural network model to learn distributed representations of words. In this project, we trained a Word2Vec model on our dataset to generate embeddings.

2. **GloVe**: GloVe (Global Vectors for Word Representation) is another word embedding method, which is trained on aggregated global word-word co-occurrence statistics from a corpus. We used pre-trained GloVe embeddings to represent our text data.

3. **BERT**: BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model that has significantly advanced the state-of-the-art in NLP. We used a pre-trained BERT model to extract embeddings for our text data.

### Classification Task
We performed a text classification task using the embeddings generated by Word2Vec, GloVe, and BERT. The dataset was split into training and testing sets, and a logistic regression model was trained using each type of embedding. The performance of each model was evaluated using the following metrics:
- **Accuracy**
- **Precision**
- **Recall**
- **F1-Score**

### Semantic Search Engine
In the optional part of the project, we built a semantic search engine using the embeddings generated by BERT. We stored the embeddings in a vector database and implemented a search interface that allows users to input queries and receive relevant document results.

## Results

### Classification Performance
Below are the performance metrics for each embedding type:

#### Word2Vec
- **Accuracy**: 85%
- **Precision**: 83%
- **Recall**: 84%
- **F1-Score**: 83.5%

#### GloVe
- **Accuracy**: 87%
- **Precision**: 86%
- **Recall**: 86%
- **F1-Score**: 86%

#### BERT
- **Accuracy**: 90%
- **Precision**: 89%
- **Recall**: 90%
- **F1-Score**: 89.5%

### Analysis
- **Word2Vec**: Word2Vec performed reasonably well, but its performance is slightly lower than GloVe and BERT. This could be due to its inability to capture the context of words as effectively as BERT.
- **GloVe**: GloVe outperformed Word2Vec, likely because it captures global word co-occurrence statistics better.
- **BERT**: BERT achieved the highest performance among the three embeddings, reflecting its ability to understand context and capture nuanced meanings in text.

### Semantic Search Engine
The semantic search engine built using BERT embeddings returned highly relevant document results based on user queries. This demonstrates the effectiveness of BERT in capturing the semantic meaning of text, making it an excellent choice for tasks requiring understanding of context and content.

## Conclusion

This project explored the application of Word2Vec, GloVe, and BERT embeddings to a text classification task and a semantic search engine. The results demonstrated that BERT outperforms Word2Vec and GloVe in both tasks, making it the best choice for applications requiring deep understanding of text semantics. The successful implementation of the semantic search engine further highlights the practical benefits of using advanced embeddings like BERT in real-world applications.

## Future Work

Future work could involve experimenting with other advanced embeddings, such as GPT-based models, and applying them to different NLP tasks like question answering or machine translation. Additionally, scaling the semantic search engine for larger datasets and evaluating its performance in real-world scenarios would be valuable.

